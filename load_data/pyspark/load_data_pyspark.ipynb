{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8469ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, MapType, ArrayType, DoubleType, DateType, TimestampType\n",
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from subprocess import PIPE, Popen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6cf67271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=0eb774c262e23e9ab35aeefb118a66a4407436a9052ee1c92e0f55025c57c470\n",
      "  Stored in directory: /Users/kovila/Library/Caches/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d18b86",
   "metadata": {},
   "source": [
    "## data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ea67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVENTS SCHEMA\n",
    "# StructField(field_name, field_type, nullable)\n",
    "events_schema = StructType([\n",
    "    \n",
    "    StructField(\"events_GLOBALEVENTID\", StringType(), True), #\n",
    "    StructField(\"SQLDATE\", StringType(), True), #\n",
    "    StructField(\"MonthYear\", StringType(), True),\n",
    "    StructField(\"Year\", StringType(), True),\n",
    "    \n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    \n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    \n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True), #\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    \n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True), #\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    \n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True), #\n",
    "    \n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    \n",
    "    StructField(\"DATEADDED\", IntegerType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8e9ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MENTIONS SCHEMA\n",
    "# StructField(field_name, field_type, nullable)\n",
    "mentions_schema = StructType([\n",
    "    \n",
    "    StructField(\"GLOBALEVENTID\", StringType(), True), #\n",
    "    StructField(\"EventTimeDate\", StringType(), True), #\n",
    "    StructField(\"MentionTimeDate\", StringType(), True), #\n",
    "    StructField(\"MentionType\", IntegerType(), True),\n",
    "    StructField(\"MentionSourceName\", StringType(), True),\n",
    "    \n",
    "    StructField(\"MentionIdentifier\", StringType(), True), #\n",
    "    StructField(\"SentenceID\", IntegerType(), True),\n",
    "    StructField(\"Actor1CharOffset\", IntegerType(), True),\n",
    "    StructField(\"Actor2CharOffset\", IntegerType(), True),\n",
    "    StructField(\"ActionCharOffset\", IntegerType(), True),\n",
    "    \n",
    "    StructField(\"InRawText\", IntegerType(), True),\n",
    "    StructField(\"Confidence\", IntegerType(), True),\n",
    "    StructField(\"MentionDocLen\", IntegerType(), True),\n",
    "    StructField(\"MentionDocTone\", FloatType(), True),\n",
    "    StructField(\"MentionDocTranslationInfo\", StringType(), True), #\n",
    "    \n",
    "    StructField(\"Extras\", StringType(), True)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe4072f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GKG SCHEMA\n",
    "# StructField(field_name, field_type, nullable)\n",
    "gkg_schema = StructType([\n",
    "    \n",
    "    StructField(\"GKGRECORDID\", StringType(), True), #\n",
    "    StructField(\"DATE\", StringType(), True), #\n",
    "    StructField(\"SourceCollectionIdentifier\", IntegerType(), True), \n",
    "    StructField(\"SourceCommonName\", StringType(), True), #\n",
    "    StructField(\"DocumentIdentifier\", StringType(), True), #\n",
    "    \n",
    "    StructField(\"Counts\", StringType(), True), \n",
    "    StructField(\"V2Counts\", StringType(), True),\n",
    "    StructField(\"Themes\", StringType(), True), #\n",
    "    #StructField(\"V2Themes\", StringType(), True), #\n",
    "    StructField(\"Locations\", StringType(), True),\n",
    "    \n",
    "    StructField(\"V2Locations\", StringType(), True),\n",
    "    StructField(\"Persons\", StringType(), True), #\n",
    "    #StructField(\"V2Persons\", StringType(), True), #\n",
    "    StructField(\"Organizations\", StringType(), True),\n",
    "    StructField(\"V2Organizations\", StringType(), True),\n",
    "    \n",
    "    StructField(\"V2Tone\", StringType(), True), # first array element only\n",
    "    StructField(\"Dates\", StringType(), True),\n",
    "    StructField(\"GCAM\", StringType(), True),\n",
    "    StructField(\"SharingImage\", StringType(), True),\n",
    "    StructField(\"RelatedImages\", StringType(), True),    \n",
    "    \n",
    "    StructField(\"SocialImageEmbeds\", StringType(), True),\n",
    "    StructField(\"SocialVideoEmbeds\", StringType(), True),\n",
    "    StructField(\"Quotations\", StringType(), True),\n",
    "    StructField(\"AllNames\", StringType(), True),\n",
    "    StructField(\"Amounts\", StringType(), True), \n",
    "    \n",
    "    StructField(\"TranslationInfo\", StringType(), True),\n",
    "    StructField(\"Extras\", StringType(), True)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8ad3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_DICTIONARY = {\n",
    "    'events':events_schema,\n",
    "    'mentions':mentions_schema,\n",
    "    'gkg':gkg_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10092cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "MASTER_FILELIST_FILEPATH = 'masterfilelist-translation.txt'\n",
    "DOWNLOAD_CSV_PATH = './gdelt_data/'\n",
    "START_URL = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "HISTORY_EXTRACTED_FILEPATH = 'history_extracted'\n",
    "HISTORY_LOADED_FILEPATH = 'history_loaded'\n",
    "EVENTS_COLUMN_HEADERS = './gdelt_columns/events_column_headers'\n",
    "YEAR = '2022'\n",
    "MONTH = '06'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869e6b6",
   "metadata": {},
   "source": [
    "## spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "739cfa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=SparkSession>\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "#SPARK = SparkSession.builder.master('local') \\\n",
    "\n",
    "#SPARK = SparkSession.builder \\\n",
    "#   .master('spark://tp-hadoop-57:7077') \\\n",
    "\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName('SparkSession') \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://tp-hadoop-50/\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://tp-hadoop-50/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#SC = SparkContext()\n",
    "#SC = SparkContext.getOrCreate().setMaster(\"spark://tp-hadoop-51:7077\")\n",
    "SC = SparkContext.getOrCreate()\n",
    "\n",
    "print(SC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03986df",
   "metadata": {},
   "source": [
    "## utility - write to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "631576f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mongodb(spark_dataframe, mongodb_database, mongodb_collection):\n",
    "    spark_dataframe.write.format('mongodb').option(\"database\",mongodb_database).option(\"collection\", mongodb_collection).mode(\"append\").save()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b809b",
   "metadata": {},
   "source": [
    "## utility - select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc0c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_df(df, df_type):\n",
    "    \n",
    "    events_columns = ['events_GLOBALEVENTID', 'SQLDATE', 'Actor1Geo_CountryCode', 'Actor2Geo_CountryCode', 'ActionGeo_CountryCode', ]\n",
    "    mentions_columns = ['GLOBALEVENTID', 'EventTimeDate', 'MentionTimeDate', 'MentionIdentifier',  'MentionDocTranslationInfo']\n",
    "    #gkg_columns = ['GKGRECORDID', 'DATE', 'SourceCommonName', 'DocumentIdentifier', 'Themes', 'V2Themes', 'Persons', 'V2Persons', 'V2Tone']\n",
    "    gkg_columns = ['GKGRECORDID', 'DATE', 'SourceCommonName', 'DocumentIdentifier', 'Themes', 'Persons', 'V2Tone']\n",
    "\n",
    "\n",
    "    selection_columns = []\n",
    "    if df_type == 'events':\n",
    "        selection_columns = events_columns\n",
    "    elif df_type == 'mentions':\n",
    "        selection_columns = mentions_columns\n",
    "    elif df_type == 'gkg':\n",
    "        selection_columns = gkg_columns\n",
    "    else:\n",
    "        raise Exception('df_type must be: events| mentions |gkg')\n",
    "        \n",
    "    transformed_df = df.select(selection_columns)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636d844",
   "metadata": {},
   "source": [
    "## get urls from master filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "956ac4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING WHY ARE SOME URLS MISSING???\n",
    "\n",
    "def get_zip_urls_from_master_filelist(year_list, month_list, day_list, zip_type, master_filelist_path='masterfilelist-translation.txt', start_url='http://data.gdeltproject.org/gdeltv2/'):\n",
    "    \n",
    "    # zip_type: events | mentions | gkg\n",
    "    zip_type_token = ''\n",
    "    if zip_type == 'events':\n",
    "        zip_type_token = '.export.'\n",
    "    elif zip_type == 'mentions':\n",
    "        zip_type_token = '.mentions.'\n",
    "    elif zip_type == 'gkg':\n",
    "        zip_type_token = '.gkg.'\n",
    "    else:\n",
    "        raise Exception('zip_type should be one of: events | mentions | gkg')\n",
    "        \n",
    "    # get masterfile list path\n",
    "    with open(master_filelist_path) as f:\n",
    "        raw_file_list = f.readlines()\n",
    "    \n",
    "    raw_file_list = [line.split() for line in raw_file_list]\n",
    "    \n",
    "    # extract zip urls from masterfile list path\n",
    "    zip_urls = []\n",
    "    for i in range(len(raw_file_list)):\n",
    "        try:\n",
    "            zip_urls.append(raw_file_list[i][2])\n",
    "        except Exception:  \n",
    "            pass\n",
    "        \n",
    "        \n",
    "    # filter specified year, month and day\n",
    "    filtered_zip_urls = []\n",
    "    \n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            if day_list is None:\n",
    "                filtered_zip_urls = filtered_zip_urls + [file for file in zip_urls if (start_url + year + month in file) and (zip_type_token in file)]\n",
    "            else:\n",
    "                for day in day_list:\n",
    "                    filtered_zip_urls = filtered_zip_urls + [file for file in zip_urls if (start_url + year + month + day in file) and (zip_type_token in file)]\n",
    "\n",
    "                \n",
    "    return filtered_zip_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e2952",
   "metadata": {},
   "source": [
    "## download and extract urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2036b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(spark_context, zip_urls, download_zip_path, start_url='http://data.gdeltproject.org/gdeltv2/'):\n",
    "    \n",
    "    extracted_filenames = []\n",
    "    \n",
    "    # create download path of it does not exist\n",
    "    if not os.path.exists(download_zip_path):\n",
    "        os.makedirs(download_zip_path)\n",
    "\n",
    "    for zip_url in zip_urls:\n",
    "        \n",
    "        downloaded_filename = zip_url.replace(start_url,'')\n",
    "        \n",
    "        # do not downloaded if already exists\n",
    "        already_downloaded = os.path.exists(download_zip_path+downloaded_filename)\n",
    "    \n",
    "        # download zip file\n",
    "        if not already_downloaded:\n",
    "            wget.download(zip_url, out=download_zip_path+downloaded_filename)\n",
    "        extracted_filename = downloaded_filename.replace('.zip','')\n",
    "    \n",
    "        # do not unzip if already exists\n",
    "        already_extracted = os.path.exists(download_zip_path+extracted_filename)\n",
    "    \n",
    "        # unzip file\n",
    "        if not already_extracted:\n",
    "            with ZipFile(download_zip_path+downloaded_filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_zip_path)    \n",
    "        \n",
    "        # add file to be available on each node\n",
    "        \n",
    "        spark_context.addFile(download_zip_path+extracted_filename)\n",
    "        \n",
    "        #spark_context.addFile('file:///home/ubuntu/gdelt_data/'+extracted_filename)\n",
    "        #put = Popen([\"../hadoop-3.3.4/bin/hadoop\", \"fs\", \"-put\", download_zip_path+extracted_filename, \"hdfs://tp-hadoop-57:9000/user/ubuntu/gdelt_data\"], stdin=PIPE, bufsize=-1)\n",
    "        # delete zip file\n",
    "        os.remove(download_zip_path+downloaded_filename)\n",
    "        \n",
    "        extracted_filenames = extracted_filenames + [extracted_filename]\n",
    "    \n",
    "    return extracted_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab460eb6",
   "metadata": {},
   "source": [
    "## spark_read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c54ff08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_csv(spark_session, spark_context, csv_filepath, csv_file_list, schema_dictionary, csv_type):\n",
    "\n",
    "    df_read = None\n",
    "    \n",
    "    # file_type: events | mentions | gkg\n",
    "    csv_type_token = ''\n",
    "    schema = None\n",
    "        \n",
    "    if csv_type == 'events':\n",
    "        csv_type_token = '.export.'\n",
    "        schema = schema_dictionary['events']\n",
    "    elif csv_type == 'mentions':\n",
    "        csv_type_token = '.mentions.'\n",
    "        schema = schema_dictionary['mentions']\n",
    "    elif csv_type == 'gkg':\n",
    "        csv_type_token = '.gkg.'\n",
    "        schema = schema_dictionary['gkg']\n",
    "    else:\n",
    "        raise Exception('csv_type should be one of: events | mentions | gkg')\n",
    "        \n",
    "    \n",
    "    for file in csv_file_list:\n",
    "        \n",
    "        # read csv\n",
    "        #df = spark_session.read.options(delimiter='\\t').csv(csv_filepath+file, schema=schema)\n",
    "        #df = spark_session.read.options(delimiter='\\t').csv('file:///home/ubuntu/gdelt_data/'+file, schema=schema)\n",
    "        #df = spark_session.read.options(delimiter='\\t').csv(SparkFiles.get('file:///home/ubuntu/gdelt_data/'+file), schema=schema)\n",
    "        \n",
    "        df = spark_session.read.options(delimiter='\\t').csv('file:///' + SparkFiles.get(file), schema=schema)\n",
    "\n",
    "        if df_read is None:\n",
    "            df_read = df.select('*')\n",
    "        else:\n",
    "            df_read = df_read.unionByName(df)\n",
    "            \n",
    "        #if os.path.exists(csv_filepath+file):\n",
    "         #   os.remove(csv_filepath+file)\n",
    "        \n",
    "    return df_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9dded1",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "835c8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(year_list, month_list, day_list, schema_dictionary, spark_session, spark_context, mongodb_database, mongodb_collection, download_csv_path, start_url):\n",
    "    \n",
    "    # GET MENTIONS\n",
    "    # download and extract mentions csv files\n",
    "    mentions_zip_urls = get_zip_urls_from_master_filelist(year_list=year_list, month_list=month_list, day_list=day_list, zip_type='mentions')\n",
    "    print(\"mentions : get_zip_urls_from_master_filelist\")\n",
    "    mentions_extracted_csvs = download_and_extract(spark_context=spark_context, zip_urls=mentions_zip_urls, download_zip_path=download_csv_path, start_url=start_url)\n",
    "    print(\"mentions : download_and_extract\")\n",
    "    \n",
    "    #stop_counter = 2\n",
    "    #counter = 0\n",
    "    \n",
    "    for mention_csv_file in mentions_extracted_csvs:\n",
    "        #counter = counter + 1\n",
    "        #if counter == stop_counter:\n",
    "         #   break\n",
    "        # read mentions csv to spark dataframe\n",
    "        mentions_df = spark_read_csv(spark_session=spark_session, spark_context=spark_context, csv_filepath=download_csv_path, csv_file_list=[mention_csv_file], schema_dictionary=schema_dictionary, csv_type='mentions')\n",
    "        print(\"mentions: spark_read_csv\")\n",
    "        \n",
    "        # TRANSFORM MENTIONS DATA\n",
    "        #mentions_df = select_columns_df(mentions_df, df_type='mentions')\n",
    "        #mentions_df = mentions_df.withColumn('MentionDocTranslationInfo', regexp_extract('MentionDocTranslationInfo', r'srclc:([a-z]{3})*', 0))\n",
    "        #mentions_df = mentions_df.withColumn('MentionDocTranslationInfo', regexp_replace('MentionDocTranslationInfo', 'srclc:', ''))\n",
    "        #mentions_df = mentions_df.withColumn('MentionDocTranslationInfo', upper('MentionDocTranslationInfo'))\n",
    "        \n",
    "        mentions_df = select_columns_df(mentions_df, df_type='mentions') \\\n",
    "        .withColumn('MentionDocTranslationInfo', regexp_extract('MentionDocTranslationInfo', r'srclc:([a-z]{3})*', 0)) \\\n",
    "        .withColumn('MentionDocTranslationInfo', regexp_replace('MentionDocTranslationInfo', 'srclc:', '')) \\\n",
    "        .withColumn('MentionDocTranslationInfo', upper('MentionDocTranslationInfo'))\n",
    "        \n",
    "        # GET EVENTS\n",
    "\n",
    "        global_event_id_filter = mentions_df.select('GLOBALEVENTID').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        global_event_timedate_filter = mentions_df.select('EventTimeDate').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        event_zip_urls = []\n",
    "        \n",
    "        for event_date in global_event_timedate_filter:\n",
    "            \n",
    "            event_year = event_date[0:4]\n",
    "            event_month = event_date[4:6]\n",
    "            event_day = event_date[6:8]\n",
    "            \n",
    "            event_zip_urls = event_zip_urls + get_zip_urls_from_master_filelist(year_list=[event_year], month_list=[event_month], day_list=[event_day], zip_type='events')\n",
    "\n",
    "        events_extracted_csvs = download_and_extract(spark_context=spark_context, zip_urls=event_zip_urls, download_zip_path=download_csv_path, start_url=start_url)\n",
    "        events_df = spark_read_csv(spark_session=spark_session, spark_context=spark_context, csv_filepath=download_csv_path, csv_file_list=events_extracted_csvs, schema_dictionary=schema_dictionary, csv_type='events')\n",
    "\n",
    "        events_df = events_df.filter(events_df.events_GLOBALEVENTID.isin(global_event_id_filter))\n",
    "\n",
    "        events_df = select_columns_df(events_df, df_type='events')\n",
    "        \n",
    "        events_df = events_df.withColumn('SQLDATE', to_date('SQLDATE', 'yyyyMMdd'))\n",
    "\n",
    "        print(\"about to join events\")\n",
    "        mentions_events_df = mentions_df.join(events_df, mentions_df.GLOBALEVENTID == events_df.events_GLOBALEVENTID, 'left')\n",
    "        \n",
    "        # CREATE NESTED EVENTS FIELD\n",
    "        \n",
    "        build_nested_event_udf = udf(lambda SQLDATE, Actor1Geo_CountryCode, Actor2Geo_CountryCode, ActionGeo_CountryCode: {\n",
    "            'SQLDATE': SQLDATE,\n",
    "            'Actor1Geo_CountryCode': Actor1Geo_CountryCode,\n",
    "            'Actor2Geo_CountryCode': Actor2Geo_CountryCode,\n",
    "            'ActionGeo_CountryCode': ActionGeo_CountryCode\n",
    "                                         \n",
    "        }, StructType([\n",
    "            StructField(\"SQLDATE\", DateType(), True),\n",
    "            StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "            StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "            StructField(\"ActionGeo_CountryCode\", StringType(), True)\n",
    "        ]))\n",
    "\n",
    "        mentions_events_df = (\n",
    "            mentions_events_df\n",
    "            .withColumn('event', build_nested_event_udf(mentions_events_df['SQLDATE'],\n",
    "                                                        mentions_events_df['Actor1Geo_CountryCode'], \n",
    "                                                        mentions_events_df['Actor2Geo_CountryCode'],\n",
    "                                                        mentions_events_df['ActionGeo_CountryCode']))\n",
    "            .drop('events_GLOBALEVENTID')\n",
    "            .drop('SQLDATE')\n",
    "            .drop('Actor1Geo_CountryCode')\n",
    "            .drop('Actor2Geo_CountryCode')\n",
    "            .drop('ActionGeo_CountryCode'))\n",
    "        \n",
    "        # GET GKG\n",
    "        mention_identifier_filter = mentions_df.select('MentionIdentifier').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "        mentions_timedate_filter = mentions_df.select('MentionTimeDate').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        gkg_zip_urls = []\n",
    "        \n",
    "        for mention_date in mentions_timedate_filter:\n",
    "            gkg_year = event_date[0:4]\n",
    "            gkg_month = event_date[4:6]\n",
    "            gkg_day = event_date[6:8]\n",
    "            \n",
    "            gkg_zip_urls = gkg_zip_urls + get_zip_urls_from_master_filelist(year_list=[gkg_year], month_list=[gkg_month], day_list=[gkg_day], zip_type='gkg')\n",
    "\n",
    "        gkg_extracted_csvs = download_and_extract(spark_context=spark_context, zip_urls=gkg_zip_urls, download_zip_path=download_csv_path, start_url=start_url)\n",
    "        gkg_df = spark_read_csv(spark_session=spark_session, spark_context=spark_context, csv_filepath=download_csv_path, csv_file_list=gkg_extracted_csvs, schema_dictionary=schema_dictionary, csv_type='gkg')        \n",
    "        \n",
    "        gkg_df = gkg_df.filter(gkg_df.DocumentIdentifier.isin(mention_identifier_filter))\n",
    "\n",
    "        # TRANSFORM GKG DATA\n",
    "        gkg_df = select_columns_df(gkg_df, df_type='gkg')\n",
    "        \n",
    "        gkg_df = gkg_df.withColumn('Persons', split('Persons', ';'))\n",
    "        gkg_df = gkg_df.withColumn('Themes', split('Themes', ';'))\n",
    "        gkg_df = gkg_df.withColumn('V2Tone', split('V2Tone', ','))\n",
    "        gkg_df = gkg_df.withColumn('V2Tone', element_at('V2Tone', 1))\n",
    "        gkg_df = gkg_df.withColumn('V2Tone',gkg_df.V2Tone.cast(DoubleType()))\n",
    "        gkg_df = gkg_df.withColumn('DATE', to_timestamp('DATE', 'yyyyMMddHHmmss'))\n",
    "        \n",
    "        print(\"about to join gkg\")\n",
    "        mentions_events_gkg_df = mentions_events_df.join(gkg_df, mentions_events_df.MentionIdentifier == gkg_df.DocumentIdentifier, 'left')\n",
    "            \n",
    "        # CREATE NESTED GKG FIELD\n",
    "\n",
    "        build_nested_gkg_udf = udf(lambda GKGRECORDID, DATE, SourceCommonName, DocumentIdentifier, Themes, Persons, V2Tone: {\n",
    "            'GKGRECORDID': GKGRECORDID,\n",
    "            'DATE': DATE,\n",
    "            'SourceCommonName': SourceCommonName,\n",
    "            'DocumentIdentifier': DocumentIdentifier,\n",
    "            'Themes': Themes,\n",
    "            #'V2Themes': V2Themes,\n",
    "            'Persons': Persons,\n",
    "            #'V2Persons': V2Persons,\n",
    "            'V2Tone': V2Tone\n",
    "                                         \n",
    "        }, StructType([\n",
    "            StructField(\"GKGRECORDID\", StringType(), True),\n",
    "            StructField(\"DATE\", TimestampType(), True),\n",
    "            StructField(\"SourceCommonName\", StringType(), True),\n",
    "            StructField(\"DocumentIdentifier\", StringType(), True),\n",
    "            StructField(\"Themes\", ArrayType(StringType(), True), True),\n",
    "            #StructField(\"V2Themes\", StringType(), True),\n",
    "            StructField(\"Persons\", ArrayType(StringType(), True), True),\n",
    "            #StructField(\"V2Persons\", StringType(), True),\n",
    "            StructField(\"V2Tone\", DoubleType(), True)\n",
    "        ]))\n",
    "\n",
    "        mentions_events_gkg_df = (\n",
    "            mentions_events_gkg_df\n",
    "            .withColumn('gkg', build_nested_gkg_udf(mentions_events_gkg_df['GKGRECORDID'],\n",
    "                                                    mentions_events_gkg_df['DATE'], \n",
    "                                                    mentions_events_gkg_df['SourceCommonName'],\n",
    "                                                    mentions_events_gkg_df['DocumentIdentifier'],\n",
    "                                                    mentions_events_gkg_df['Themes'],\n",
    "                                                    #mentions_events_gkg_df['V2Themes'],\n",
    "                                                    mentions_events_gkg_df['Persons'],\n",
    "                                                    #mentions_events_gkg_df['V2Persons'],\n",
    "                                                    mentions_events_gkg_df['V2Tone']))\n",
    "            .drop('GKGRECORDID')\n",
    "            .drop('DATE')\n",
    "            .drop('SourceCommonName')\n",
    "            .drop('DocumentIdentifier')\n",
    "            .drop('Themes')\n",
    "            .drop('Persons')\n",
    "            .drop('V2Tone'))\n",
    "\n",
    "        # FORMAT TIME DATE #\n",
    "\n",
    "        mentions_events_gkg_df = mentions_events_gkg_df.withColumn('EventTimeDate', to_timestamp('EventTimeDate', 'yyyyMMddHHmmss')) \\\n",
    "        .withColumn('MentionTimeDate', to_timestamp('MentionTimeDate', 'yyyyMMddHHmmss'))\n",
    "        \n",
    "        # WRITE TO MONGODB #\n",
    "        load_mongodb(mentions_events_gkg_df, mongodb_database=mongodb_database, mongodb_collection=mongodb_collection)\n",
    "        \n",
    "        # DELETE CSVs\n",
    "        for mention_csv in mentions_extracted_csvs:\n",
    "            if os.path.exists(download_csv_path+mention_csv):\n",
    "                os.remove(download_csv_path+mention_csv)\n",
    "        for event_csv in events_extracted_csvs:\n",
    "            if os.path.exists(download_csv_path+event_csv):\n",
    "                os.remove(download_csv_path+event_csv)\n",
    "        for gkg_csv in gkg_extracted_csvs:\n",
    "            if os.path.exists(download_csv_path+gkg_csv):\n",
    "                os.remove(download_csv_path+gkg_csv)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "765cafdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_zip_urls_from_master_filelist\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "download_and_extract\n",
      "read file from spark context\n",
      "spark_read_csv\n",
      "select_columns_df\n",
      "global_event_id_filter\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n",
      "add file spark context\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-976aa4c3f41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# started: 23:21-ABORTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m load_data(year_list=['2022'], \n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mmonth_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mday_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mschema_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCHEMA_DICTIONARY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-78947da70398>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(year_list, month_list, day_list, schema_dictionary, spark_session, spark_context, mongodb_database, mongodb_collection, download_csv_path, start_url)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mevent_zip_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_zip_urls_from_master_filelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'events'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mevents_extracted_csvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_urls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent_zip_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_zip_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mrow_events_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_read_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_filepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevents_extracted_csvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'events'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-360199a8d1f7>\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(spark_context, zip_urls, download_zip_path, start_url)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# download zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malready_downloaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_zip_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdownloaded_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mextracted_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownloaded_filename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/wget.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mbinurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mulib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    543\u001b[0m                                   '_open', req)\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# started: 23:21-ABORTED\n",
    "load_data(year_list=['2022'], \n",
    "          month_list=['12'],\n",
    "          day_list=['01'], \n",
    "          schema_dictionary=SCHEMA_DICTIONARY, \n",
    "          spark_session = SPARK, \n",
    "          spark_context = SC,\n",
    "          mongodb_database = 'gdelt',\n",
    "          mongodb_collection = 'datamodel1', \n",
    "          download_csv_path = '../gdelt_data/', \n",
    "          start_url = 'http://data.gdeltproject.org/gdeltv2/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c5161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
