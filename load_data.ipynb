{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8469ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, MapType\n",
    "#from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6cf67271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=0eb774c262e23e9ab35aeefb118a66a4407436a9052ee1c92e0f55025c57c470\n",
      "  Stored in directory: /Users/kovila/Library/Caches/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a8b06",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- delete csv option\n",
    "- delete zips option\n",
    "- create history files if they do not exist\n",
    "- add columns to data\n",
    "- single file option for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10092cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "MASTER_FILELIST_FILEPATH = 'masterfilelist-translation.txt'\n",
    "DOWNLOAD_CSV_PATH = './gdelt_data/'\n",
    "START_URL = 'http://data.gdeltproject.org/gdeltv2/'\n",
    "HISTORY_EXTRACTED_FILEPATH = 'history_extracted'\n",
    "HISTORY_LOADED_FILEPATH = 'history_loaded'\n",
    "EVENTS_COLUMN_HEADERS = './gdelt_columns/events_column_headers'\n",
    "YEAR = '2022'\n",
    "MONTH = '06'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636d844",
   "metadata": {},
   "source": [
    "## get urls from master filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956ac4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING WHY ARE SOME URLS MISSING???\n",
    "\n",
    "def get_zip_urls_from_master_filelist(year_list, month_list, day_list, zip_type, master_filelist_path='masterfilelist-translation.txt', start_url='http://data.gdeltproject.org/gdeltv2/'):\n",
    "    \n",
    "    # zip_type: events | mentions | gkg\n",
    "    zip_type_token = ''\n",
    "    if zip_type == 'events':\n",
    "        zip_type_token = '.export.'\n",
    "    elif zip_type == 'mentions':\n",
    "        zip_type_token = '.mentions.'\n",
    "    elif zip_type == 'gkg':\n",
    "        zip_type_token = '.gkg.'\n",
    "    else:\n",
    "        raise Exception('zip_type should be one of: events | mentions | gkg')\n",
    "        \n",
    "    # get masterfile list path\n",
    "    with open(master_filelist_path) as f:\n",
    "        raw_file_list = f.readlines()\n",
    "    \n",
    "    raw_file_list = [line.split() for line in raw_file_list]\n",
    "    \n",
    "    # extract zip urls from masterfile list path\n",
    "    zip_urls = []\n",
    "    for i in range(len(raw_file_list)):\n",
    "        try:\n",
    "            zip_urls.append(raw_file_list[i][2])\n",
    "        except Exception:  \n",
    "            pass\n",
    "        \n",
    "        \n",
    "    # filter specified year, month and day\n",
    "    filtered_zip_urls = []\n",
    "    \n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            if day_list is None:\n",
    "                filtered_zip_urls = filtered_zip_urls + [file for file in zip_urls if (start_url + year + month in file) and (zip_type_token in file)]\n",
    "            else:\n",
    "                for day in day_list:\n",
    "                    filtered_zip_urls = filtered_zip_urls + [file for file in zip_urls if (start_url + year + month + day in file) and (zip_type_token in file)]\n",
    "\n",
    "                \n",
    "    return filtered_zip_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e2952",
   "metadata": {},
   "source": [
    "## download and extract urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2036b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(zip_urls, download_zip_path, start_url='http://data.gdeltproject.org/gdeltv2/'):\n",
    "    \n",
    "    extracted_filenames = []\n",
    "    \n",
    "    # create download path of it does not exist\n",
    "    if not os.path.exists(download_zip_path):\n",
    "        os.makedirs(download_zip_path)\n",
    "\n",
    "    for zip_url in zip_urls:\n",
    "        \n",
    "        downloaded_filename = zip_url.replace(start_url,'')\n",
    "        \n",
    "        # do not downloaded if already exists\n",
    "        already_downloaded = os.path.exists(download_zip_path+downloaded_filename)\n",
    "    \n",
    "        # download zip file\n",
    "        if not already_downloaded:\n",
    "            wget.download(zip_url, out=download_zip_path+downloaded_filename)\n",
    "        extracted_filename = downloaded_filename.replace('.zip','')\n",
    "    \n",
    "        # do not unzip if already exists\n",
    "        already_extracted = os.path.exists(download_zip_path+extracted_filename)\n",
    "    \n",
    "        # unzip file\n",
    "        if not already_extracted:\n",
    "            with ZipFile(download_zip_path+downloaded_filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_zip_path)    \n",
    "        \n",
    "        # delete zip file\n",
    "        os.remove(download_zip_path+downloaded_filename)\n",
    "        \n",
    "        extracted_filenames = extracted_filenames + [extracted_filename]\n",
    "    \n",
    "    return extracted_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63cb70",
   "metadata": {},
   "source": [
    "## data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a04b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVENTS SCHEMA\n",
    "# StructField(field_name, field_type, nullable)\n",
    "events_schema = StructType([\n",
    "    \n",
    "    StructField(\"GLOBALEVENTID\", StringType(), True), #\n",
    "    StructField(\"SQLDATE\", StringType(), True), #\n",
    "    StructField(\"MonthYear\", StringType(), True),\n",
    "    StructField(\"Year\", StringType(), True),\n",
    "    \n",
    "    StructField(\"FractionDate\", FloatType(), True),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    \n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", FloatType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    \n",
    "    StructField(\"AvgTone\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True), #\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    \n",
    "    StructField(\"Actor1Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    \n",
    "    StructField(\"Actor2Geo_FullName\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True), #\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", FloatType(), True),\n",
    "    \n",
    "    StructField(\"Actor2Geo_Long\", FloatType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_FullName\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True), #\n",
    "    \n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM2Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_Long\", FloatType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    \n",
    "    StructField(\"DATEADDED\", IntegerType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "200c84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MENTIONS SCHEMA\n",
    "# StructField(field_name, field_type, nullable)\n",
    "mentions_schema = StructType([\n",
    "    \n",
    "    StructField(\"GLOBALEVENTID\", StringType(), True), #\n",
    "    StructField(\"EventTimeDate\", StringType(), True), #\n",
    "    StructField(\"MentionTimeDate\", StringType(), True), #\n",
    "    StructField(\"MentionType\", IntegerType(), True),\n",
    "    StructField(\"MentionSourceName\", StringType(), True),\n",
    "    \n",
    "    StructField(\"MentionIdentifier\", StringType(), True), #\n",
    "    StructField(\"SentenceID\", IntegerType(), True),\n",
    "    StructField(\"Actor1CharOffset\", IntegerType(), True),\n",
    "    StructField(\"Actor2CharOffset\", IntegerType(), True),\n",
    "    StructField(\"ActionCharOffset\", IntegerType(), True),\n",
    "    \n",
    "    StructField(\"InRawText\", IntegerType(), True),\n",
    "    StructField(\"Confidence\", IntegerType(), True),\n",
    "    StructField(\"MentionDocLen\", IntegerType(), True),\n",
    "    StructField(\"MentionDocTone\", FloatType(), True),\n",
    "    StructField(\"MentionDocTranslationInfo\", StringType(), True), #\n",
    "    \n",
    "    StructField(\"Extras\", StringType(), True)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7aec386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GKG SCHEMA\n",
    "# StructField(field_name, field_type, nullable)\n",
    "gkg_schema = StructType([\n",
    "    \n",
    "    StructField(\"GKGRECORDID\", StringType(), True), #\n",
    "    StructField(\"DATE\", StringType(), True), #\n",
    "    StructField(\"SourceCollectionIdentifier\", IntegerType(), True), \n",
    "    StructField(\"SourceCommonName\", StringType(), True), #\n",
    "    StructField(\"DocumentIdentifier\", StringType(), True), #\n",
    "    \n",
    "    StructField(\"Counts\", StringType(), True), \n",
    "    StructField(\"V2Counts\", StringType(), True),\n",
    "    StructField(\"Themes\", StringType(), True), #\n",
    "    StructField(\"V2Themes\", StringType(), True), #\n",
    "    StructField(\"Locations\", StringType(), True),\n",
    "    \n",
    "    StructField(\"V2Locations\", StringType(), True),\n",
    "    StructField(\"Persons\", StringType(), True), #\n",
    "    StructField(\"V2Persons\", StringType(), True), #\n",
    "    StructField(\"Organizations\", StringType(), True),\n",
    "    StructField(\"V2Organizations\", StringType(), True),\n",
    "    \n",
    "    StructField(\"V2Tone\", StringType(), True), # first array element only\n",
    "    StructField(\"Dates\", StringType(), True),\n",
    "    StructField(\"GCAM\", StringType(), True),\n",
    "    StructField(\"SharingImage\", StringType(), True),\n",
    "    StructField(\"RelatedImages\", StringType(), True),    \n",
    "    \n",
    "    StructField(\"SocialImageEmbeds\", StringType(), True),\n",
    "    StructField(\"SocialVideoEmbeds\", StringType(), True),\n",
    "    StructField(\"Quotations\", StringType(), True),\n",
    "    StructField(\"AllNames\", StringType(), True),\n",
    "    StructField(\"Amounts\", StringType(), True), \n",
    "    \n",
    "    StructField(\"TranslationInfo\", StringType(), True),\n",
    "    StructField(\"Extras\", StringType(), True)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20d66629",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_DICTIONARY = {\n",
    "    'events':events_schema,\n",
    "    'mentions':mentions_schema,\n",
    "    'gkg':gkg_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a4049",
   "metadata": {},
   "source": [
    "## spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41a5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "SPARK = SparkSession.builder.master('local') \\\n",
    "    .appName('SparkSession') \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://tp-hadoop-50/\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://tp-hadoop-50/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab460eb6",
   "metadata": {},
   "source": [
    "## spark_read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54ff08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_csv(spark_session, csv_filepath, csv_file_list, schema_dictionary, csv_type):\n",
    "\n",
    "    df_read = None\n",
    "    \n",
    "    for file in csv_file_list:\n",
    "        \n",
    "        # file_type: events | mentions | gkg\n",
    "        csv_type_token = ''\n",
    "        schema = None\n",
    "        \n",
    "        if csv_type == 'events':\n",
    "            csv_type_token = '.export.'\n",
    "            schema = schema_dictionary['events']\n",
    "        elif csv_type == 'mentions':\n",
    "            csv_type_token = '.mentions.'\n",
    "            schema = schema_dictionary['mentions']\n",
    "        elif csv_type == 'gkg':\n",
    "            csv_type_token = '.gkg.'\n",
    "            schema = schema_dictionary['gkg']\n",
    "        else:\n",
    "            raise Exception('csv_type should be one of: events | mentions | gkg')\n",
    "        \n",
    "        # read csv\n",
    "        df = spark_session.read.options(delimiter='\\t').csv(csv_filepath+file, schema=schema)\n",
    "        \n",
    "        if df_read is None:\n",
    "            df_read = df.select('*')\n",
    "        else:\n",
    "            df_read = df_read.unionByName(df)\n",
    "        \n",
    "    return df_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93551d4",
   "metadata": {},
   "source": [
    "## transform events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45f283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_df(df, df_type):\n",
    "    \n",
    "    events_columns = ['GLOBALEVENTID', 'SQLDATE', 'Actor1Geo_CountryCode', 'Actor2Geo_CountryCode', 'Actor1Geo_CountryCode', ]\n",
    "    mentions_columns = ['GLOBALEVENTID', 'EventTimeDate', 'MentionTimeDate', 'MentionIdentifier',  'MentionDocTranslationInfo']\n",
    "    gkg_columns = ['GKGRECORDID', 'DATE', 'SourceCommonName', 'DocumentIdentifier', 'Themes', 'V2Themes', 'Persons', 'V2Persons', 'V2Tone']\n",
    "\n",
    "    selection_columns = []\n",
    "    if df_type == 'events':\n",
    "        selection_columns = events_columns\n",
    "    elif df_type == 'mentions':\n",
    "        selection_columns = mentions_columns\n",
    "    elif df_type == 'gkg':\n",
    "        selection_columns = gkg_columns\n",
    "    else:\n",
    "        raise Exception('df_type must be: events| mentions |gkg')\n",
    "        \n",
    "    transformed_df = df.select(selection_columns)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61795ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_events_data(events_df):\n",
    "    \n",
    "    events_columns = ['GLOBALEVENTID', 'SQLDATE', 'ActionGeo_CountryCode']\n",
    "    transformed_df = events_df.select(events_columns)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d8ab3",
   "metadata": {},
   "source": [
    "## write spark dataframe to mongodb collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a40f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mongodb(spark_dataframe, mongodb_database, mongodb_collection):\n",
    "    spark_dataframe.write.format('mongodb').option(\"database\",mongodb_database).option(\"collection\", mongodb_collection).mode(\"append\").save()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9dded1",
   "metadata": {},
   "source": [
    "## ETL EVENTS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50cbcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(year_list, month_list, day_list, schema_dictionary, spark_session, mongodb_database, mongodb_collection, download_csv_path, start_url):\n",
    "    \n",
    "    # GET MENTIONS\n",
    "    # download and extract mentions csv files\n",
    "    mentions_zip_urls = get_zip_urls_from_master_filelist(year_list=year_list, month_list=month_list, day_list=day_list, zip_type='mentions')\n",
    "    mentions_extracted_csvs = download_and_extract(mentions_zip_urls, download_zip_path=download_csv_path, start_url=start_url)\n",
    "    # read mentions csv to spark dataframe\n",
    "    mentions_df = spark_read_csv(spark_session=spark_session, csv_filepath=download_csv_path, csv_file_list=[mentions_extracted_csvs], schema_dictionary=schema_dictionary, csv_type='mentions')\n",
    "    mentions_df = select_columns_df(mentions_df, df_type='mentions')\n",
    "    \n",
    "    # GET EVENTS\n",
    "    # get event date filter\n",
    "    event_dates = mentions_df.select('EventTimeDate').distinct()\n",
    "    event_dates = event_dates.withColumn('year', substring('EventTimeDate', 1,4))\n",
    "    event_dates = event_dates.withColumn('month', substring('EventTimeDate', 5,2))\n",
    "    event_dates = event_dates.withColumn('day', substring('EventTimeDate', 7,2))\n",
    "        \n",
    "    # warning: section of code not parallelizable: pandas dataframe!!!\n",
    "    # not a transformation on event_dates dataframe\n",
    "    # download event files\n",
    "    event_dates_pandas = event_dates.toPandas()\n",
    "    event_zip_urls = []\n",
    "    for index, row in event_dates_pandas.iterrows():    \n",
    "        event_zip_urls = event_zip_urls + get_zip_urls_from_master_filelist(year_list=[row['year']], month_list=[row['month']], day_list=[row['day']], zip_type='events')\n",
    "        \n",
    "    events_extracted_csvs = download_and_extract(event_zip_urls, download_zip_path=download_csv_path, start_url=start_url)\n",
    "        \n",
    "    events_df = spark_read_csv(spark_session=spark_session, csv_filepath=download_csv_path, csv_file_list=events_extracted_csvs, schema_dictionary=schema_dictionary, csv_type='events')\n",
    "    events_df = select_columns_df(events_df, df_type='events')\n",
    "\n",
    "    # JOIN MENTIONS AND EVENTS\n",
    "    mentions_events_df = mentions_df.join(events_df, mentions_df.GLOBALEVENTID == events_df.GLOBALEVENTID, 'left')\n",
    "\n",
    "\n",
    "    # GET GKG\n",
    "    # get event date filter\n",
    "    gkg_dates = mentions_df.select('MentionTimeDate').distinct()\n",
    "    gkg_dates = gkg_dates.withColumn('year', substring('MentionTimeDate', 1,4))\n",
    "    gkg_dates = gkg_dates.withColumn('month', substring('MentionTimeDate', 5,2))\n",
    "    gkg_dates = gkg_dates.withColumn('day', substring('MentionTimeDate', 7,2))\n",
    "        \n",
    "    # warning: section of code not parallelizable: pandas dataframe!!!\n",
    "    # not a transformation on event_dates dataframe\n",
    "    # download event files\n",
    "    gkg_dates_pandas = gkg_dates.toPandas()\n",
    "    gkg_zip_urls = []\n",
    "    for index, row in gkg_dates_pandas.iterrows():    \n",
    "        gkg_zip_urls = gkg_zip_urls + get_zip_urls_from_master_filelist(year_list=[row['year']], month_list=[row['month']], day_list=[row['day']], zip_type='gkg')\n",
    "        \n",
    "    gkg_extracted_csvs = download_and_extract(gkg_zip_urls, download_zip_path=download_csv_path, start_url=start_url)\n",
    "        \n",
    "    gkg_df = spark_read_csv(spark_session=spark_session, csv_filepath=download_csv_path, csv_file_list=gkg_extracted_csvs, schema_dictionary=schema_dictionary, csv_type='gkg')\n",
    "    gkg_df = select_columns_df(gkg_df, df_type='gkg')\n",
    "    \n",
    "    # JOIN MENTIONS AND GKG\n",
    "    mentions_events_gkg_df = mentions_df.join(events_df, mentions_df.GLOBALEVENTID == events_df.GLOBALEVENTID, 'left')\n",
    "\n",
    "\n",
    "    \n",
    "    return mentions_df, events_df, gkg_df, mentions_events_gkg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_df = load_data(year_list=['2022'], \n",
    "                        month_list=['01'], \n",
    "                        day_list=['01'], \n",
    "                        schema_dictionary=SCHEMA_DICTIONARY, \n",
    "                        spark_session = SPARK, \n",
    "                        mongodb_database = None, \n",
    "                        mongodb_collection = None, \n",
    "                        download_csv_path = './gdelt_data/', \n",
    "                        start_url = 'http://data.gdeltproject.org/gdeltv2/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2bfb28da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'EventTimeDate[0]'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_df['EventTimeDate'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "636b2747",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression cannot contain assignment, perhaps you meant \"==\"? (<ipython-input-84-294a6c78d829>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-84-294a6c78d829>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    mentions_df.foreach(lambda row:\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expression cannot contain assignment, perhaps you meant \"==\"?\n"
     ]
    }
   ],
   "source": [
    "test = ''\n",
    "mentions_df.foreach(lambda row: \n",
    "    test = row['EventTimeDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "796e9a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+---------------+--------------------+-------------------------+\n",
      "|GLOBALEVENTID| EventTimeDate|MentionTimeDate|   MentionIdentifier|MentionDocTranslationInfo|\n",
      "+-------------+--------------+---------------+--------------------+-------------------------+\n",
      "|   1021421880|20220101000000| 20220101000000|http://www.pasien...|     srclc:lit;eng:GT-...|\n",
      "|   1021421881|20220101000000| 20220101000000|https://aminata.c...|     srclc:fra;eng:Mos...|\n",
      "|   1021421152|20220101001500| 20220101000000|https://www.caden...|     srclc:spa;eng:Mos...|\n",
      "|   1021421882|20220101000000| 20220101000000|https://www.caden...|     srclc:spa;eng:Mos...|\n",
      "|   1021421883|20220101000000| 20220101000000|https://www.em.co...|     srclc:por;eng:GT-...|\n",
      "|   1021421884|20220101000000| 20220101000000|https://www.em.co...|     srclc:por;eng:GT-...|\n",
      "|   1021421885|20220101000000| 20220101000000|http://news.abidj...|     srclc:fra;eng:Mos...|\n",
      "|   1021421885|20220101000000| 20220101000000|https://www.fratm...|     srclc:fra;eng:Mos...|\n",
      "|   1021421885|20220101000000| 20220101000000|https://news.abid...|     srclc:fra;eng:Mos...|\n",
      "|   1021421886|20220101000000| 20220101000000|https://news.abid...|     srclc:fra;eng:Mos...|\n",
      "|   1021421887|20220101000000| 20220101000000|https://laverdadn...|     srclc:spa;eng:Mos...|\n",
      "|   1021421888|20220101000000| 20220101000000|https://culturage...|     srclc:spa;eng:Mos...|\n",
      "|   1021421889|20220101000000| 20220101000000|https://www.hoydi...|     srclc:spa;eng:Mos...|\n",
      "|   1021421890|20220101000000| 20220101000000|https://nr2.com.u...|     srclc:rus;eng:Mos...|\n",
      "|   1021421891|20220101000000| 20220101000000|https://www.lecou...|     srclc:fra;eng:Mos...|\n",
      "|   1021421305|20220101001500| 20220101000000|https://rancherit...|     srclc:spa;eng:Mos...|\n",
      "|   1021419999|20220101000000| 20220101000000|https://www.hoydi...|     srclc:spa;eng:Mos...|\n",
      "|   1021421892|20220101000000| 20220101000000|https://www.minut...|     srclc:spa;eng:Mos...|\n",
      "|   1021421893|20220101000000| 20220101000000|https://www.lecou...|     srclc:fra;eng:Mos...|\n",
      "|   1021421894|20220101000000| 20220101000000|https://www.facen...|     srclc:rus;eng:Mos...|\n",
      "+-------------+--------------+---------------+--------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61957d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = mentions_df.select('EventTimeDate').distinct()\n",
    "test = test.withColumn('year', substring('EventTimeDate', 1,4))\n",
    "test = test.withColumn('month', substring('EventTimeDate', 5,2))\n",
    "test = test.withColumn('day', substring('EventTimeDate', 7,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2c70fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pandas = test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a9c32cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2021 01\n",
      "2021 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2021 12\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2021 01\n",
      "2021 12\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2022 01\n",
      "2021 12\n",
      "2022 01\n"
     ]
    }
   ],
   "source": [
    "for index, row in test_pandas.iterrows():\n",
    "    print(row['year'], row['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a2175ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+---------------+--------------------+-------------------------+----+-----+\n",
      "|GLOBALEVENTID| EventTimeDate|MentionTimeDate|   MentionIdentifier|MentionDocTranslationInfo|year|month|\n",
      "+-------------+--------------+---------------+--------------------+-------------------------+----+-----+\n",
      "|   1021421880|20220101000000| 20220101000000|http://www.pasien...|     srclc:lit;eng:GT-...|2022|   01|\n",
      "|   1021421881|20220101000000| 20220101000000|https://aminata.c...|     srclc:fra;eng:Mos...|2022|   01|\n",
      "|   1021421152|20220101001500| 20220101000000|https://www.caden...|     srclc:spa;eng:Mos...|2022|   01|\n",
      "|   1021421882|20220101000000| 20220101000000|https://www.caden...|     srclc:spa;eng:Mos...|2022|   01|\n",
      "|   1021421883|20220101000000| 20220101000000|https://www.em.co...|     srclc:por;eng:GT-...|2022|   01|\n",
      "|   1021421884|20220101000000| 20220101000000|https://www.em.co...|     srclc:por;eng:GT-...|2022|   01|\n",
      "|   1021421885|20220101000000| 20220101000000|http://news.abidj...|     srclc:fra;eng:Mos...|2022|   01|\n",
      "|   1021421885|20220101000000| 20220101000000|https://www.fratm...|     srclc:fra;eng:Mos...|2022|   01|\n",
      "|   1021421885|20220101000000| 20220101000000|https://news.abid...|     srclc:fra;eng:Mos...|2022|   01|\n",
      "|   1021421886|20220101000000| 20220101000000|https://news.abid...|     srclc:fra;eng:Mos...|2022|   01|\n",
      "|   1021421887|20220101000000| 20220101000000|https://laverdadn...|     srclc:spa;eng:Mos...|2022|   01|\n",
      "|   1021421888|20220101000000| 20220101000000|https://culturage...|     srclc:spa;eng:Mos...|2022|   01|\n",
      "|   1021421889|20220101000000| 20220101000000|https://www.hoydi...|     srclc:spa;eng:Mos...|2022|   01|\n",
      "|   1021421890|20220101000000| 20220101000000|https://nr2.com.u...|     srclc:rus;eng:Mos...|2022|   01|\n",
      "|   1021421891|20220101000000| 20220101000000|https://www.lecou...|     srclc:fra;eng:Mos...|2022|   01|\n",
      "|   1021421305|20220101001500| 20220101000000|https://rancherit...|     srclc:spa;eng:Mos...|2022|   01|\n",
      "|   1021419999|20220101000000| 20220101000000|https://www.hoydi...|     srclc:spa;eng:Mos...|2022|   01|\n",
      "|   1021421892|20220101000000| 20220101000000|https://www.minut...|     srclc:spa;eng:Mos...|2022|   01|\n",
      "|   1021421893|20220101000000| 20220101000000|https://www.lecou...|     srclc:fra;eng:Mos...|2022|   01|\n",
      "|   1021421894|20220101000000| 20220101000000|https://www.facen...|     srclc:rus;eng:Mos...|2022|   01|\n",
      "+-------------+--------------+---------------+--------------------+-------------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5da703",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.withColumn(\"year\", col(\"EventTimeDate\").str.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c66df753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|EventTimeDate| day|\n",
      "+-------------+----+\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "|         null|null|\n",
      "+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_dates.withColumn(\"EventTimeDate\",to_timestamp(col(\"EventTimeDate\"))).withColumn(\"day\", date_format(col(\"EventTimeDate\"), \"D\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82b2c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dates = mentions_df.select('EventTimeDate').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4e18d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'EventTimeDate'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Seqno\",\"Name\", upper(df.Name)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c04c80d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-3e729d9308bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEventTimeDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'YYYY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "date_format(mentions_df.EventTimeDate, 'YYYY').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b23d4289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "| EventTimeDate|\n",
      "+--------------+\n",
      "|20220101001500|\n",
      "|20220101000000|\n",
      "|20211231050000|\n",
      "|20210101031500|\n",
      "|20211202230000|\n",
      "|20220101003000|\n",
      "|20211202124500|\n",
      "|20220101010000|\n",
      "|20220101004500|\n",
      "|20211202113000|\n",
      "|20210101094500|\n",
      "|20211231230000|\n",
      "|20220101011500|\n",
      "|20220101013000|\n",
      "|20210101040000|\n",
      "|20210101001500|\n",
      "|20220101014500|\n",
      "|20220101020000|\n",
      "|20220101021500|\n",
      "|20220101023000|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fc2bddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-ad3011b44630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmentions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EventTimeDate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "mentions_df.select(mentions_df(\"EventTimeDate\")).distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cd93bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_df_pandas = mentions_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6a8b0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_df_pandas['EventTimeDate'].isnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699df8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "         ..\n",
       "49023   NaN\n",
       "49024   NaN\n",
       "49025   NaN\n",
       "49026   NaN\n",
       "49027   NaN\n",
       "Name: EventTimeDate, Length: 49028, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_event_time_date = mentions_df.toPandas()['EventTimeDate']\n",
    "mentions_event_time_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96bff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_events(year_list, month_list, day_list, schema_dictionary, spark_session, mongodb_database, mongodb_collection, download_csv_path, start_url):\n",
    "    \n",
    "    # get events urls from master filelist\n",
    "    zip_urls = get_zip_urls_from_master_filelist(year_list=year_list, month_list=month_list, day_list=day_list, zip_type='events')\n",
    "\n",
    "    # download and extract csv\n",
    "    extracted_csvs = download_and_extract(zip_urls, download_zip_path=download_csv_path, start_url=start_url)\n",
    "\n",
    "    # read csv to spark dataframe\n",
    "    events_df = spark_read_csv(spark_session=spark_session, csv_filepath=download_csv_path, csv_file_list=extracted_csvs, schema_dictionary=schema_dictionary, csv_type='events')\n",
    "\n",
    "    # transform events data\n",
    "    events_df = transform_events_data(events_df)\n",
    "    \n",
    "    # load events data to mongodb\n",
    "    load_mongodb(events_df, mongodb_database=mongodb_database, mongodb_collection=mongodb_collection)\n",
    "    \n",
    "    # delete zip file\n",
    "    os.remove(download_zip_path+extracted_csvs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07ae4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_urls = get_zip_urls_from_master_filelist(year_list=['2022'], month_list=['01'], day_list=['01','02','03'], zip_type='events')\n",
    "extracted_csvs = download_and_extract(zip_urls, download_zip_path='./gdelt_data/', start_url='http://data.gdeltproject.org/gdeltv2/')\n",
    "events_df = spark_read_csv(spark_session=SPARK, csv_filepath='./gdelt_data/', csv_file_list=extracted_csvs, schema_dictionary=SCHEMA_DICTIONARY, csv_type='events')\n",
    "\n",
    "#events_df = transform_events_data(events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cf5dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_urls = get_zip_urls_from_master_filelist(year_list=['2022'], month_list=['01'], day_list=['01','02','03'], zip_type='gkg')\n",
    "extracted_csvs = download_and_extract(zip_urls, download_zip_path='./gdelt_data/', start_url='http://data.gdeltproject.org/gdeltv2/')\n",
    "gkg_df = spark_read_csv(spark_session=SPARK, csv_filepath='./gdelt_data/', csv_file_list=extracted_csvs, schema_dictionary=SCHEMA_DICTIONARY, csv_type='gkg')\n",
    "\n",
    "#events_df = transform_events_data(events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dda4f884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6939b463",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'values_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-061d72d04801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevents_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'events_GLOBALEVENTID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'values_count'"
     ]
    }
   ],
   "source": [
    "df1 = events_df.toPandas()['events_GLOBALEVENTID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7ed3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = events_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48ca1d75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'values_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-bceef5e06291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'events_GLOBALEVENTID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'values_count'"
     ]
    }
   ],
   "source": [
    "df1['events_GLOBALEVENTID'].values_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929193e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3787.save.\n: java.lang.ClassNotFoundException: \nFailed to find data source: mongodb. Please find packages at\nhttps://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:587)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:675)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:725)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:864)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:661)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$$Lambda$1549/1171936102.apply(Unknown Source)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:661)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$$Lambda$1548/284708524.apply(Unknown Source)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-df7eba92ee65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0metl_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2022'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'02'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'03'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCHEMA_DICTIONARY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSPARK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongodb_database\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongodb_collection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'events'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./gdelt_data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://data.gdeltproject.org/gdeltv2/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3f13d2b2828f>\u001b[0m in \u001b[0;36metl_events\u001b[0;34m(year_list, month_list, day_list, schema_dictionary, spark_session, mongodb_database, mongodb_collection, download_csv_path, start_url)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# load events data to mongodb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mload_mongodb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongodb_database\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongodb_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongodb_collection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongodb_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e1c139305439>\u001b[0m in \u001b[0;36mload_mongodb\u001b[0;34m(spark_dataframe, mongodb_database, mongodb_collection)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mongodb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongodb_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongodb_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mspark_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mongodb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"database\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmongodb_database\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongodb_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3787.save.\n: java.lang.ClassNotFoundException: \nFailed to find data source: mongodb. Please find packages at\nhttps://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:587)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:675)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:725)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:864)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:661)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$$Lambda$1549/1171936102.apply(Unknown Source)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:661)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$$Lambda$1548/284708524.apply(Unknown Source)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "etl_events(year_list=['2022'], month_list=['01'], day_list=['01','02','03'], schema_dictionary=SCHEMA_DICTIONARY, spark_session=SPARK, mongodb_database='test', mongodb_collection='events', download_csv_path='./gdelt_data/', start_url='http://data.gdeltproject.org/gdeltv2/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f248bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mentions_data(spark_session, mentions_df, mongodb_events_collection, mongodb_database):\n",
    "    \n",
    "    # filter mentions columns\n",
    "    mentions_columns = ['mentions_GLOBALEVENTID', 'MentionIdentifier', 'MentionDocTranslationInfo']\n",
    "    mentions_columns = ['mentions_GLOBALEVENTID', 'MentionIdentifier']\n",
    "    \n",
    "    mentions_df = mentions_df.select(mentions_columns)\n",
    "    \n",
    "    # find event GLOBALEVENTID from mongodb events collection\n",
    "    events_df = spark_session.read.format(\"mongodb\").option(\"database\",mongodb_database).option(\"collection\", mongodb_events_collection).load()\n",
    "    \n",
    "    # join events and mentions\n",
    "    mentions_events_df = mentions_df.join(events_df, events_df.events_GLOBALEVENTID == mentions_df.mentions_GLOBALEVENTID, 'left')\n",
    "    \n",
    "    # create nested mentions df\n",
    "    build_nested_event_udf = udf(lambda SQLDATE, ActionGeo_CountryCode: {\n",
    "        'SQLDATE': SQLDATE,\n",
    "        'ActionGeo_CountryCode': ActionGeo_CountryCode\n",
    "    }, MapType(StringType(), StringType()))\n",
    "\n",
    "    mentions_events_df = (\n",
    "        mentions_events_df\n",
    "        .withColumn('event_fields', build_nested_event_udf(mentions_events_df['SQLDATE'], mentions_events_df['ActionGeo_CountryCode']))\n",
    "        .drop('SQLDATE')\n",
    "        .drop('ActionGeo_CountryCode')\n",
    "        .drop('events_GLOBALEVENTID')\n",
    "    )\n",
    "    \n",
    "    return mentions_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_mentions(year_list, month_list, day_list, schema_dictionary, spark_session, mongodb_database, mongodb_mentions_collection, mongodb_events_collection, download_csv_path, start_url):\n",
    "    \n",
    "    # get events urls from master filelist\n",
    "    zip_urls = get_zip_urls_from_master_filelist(year_list=year_list, month_list=month_list, day_list=day_list, zip_type='mentions')\n",
    "\n",
    "    # download and extract csv\n",
    "    extracted_csvs = download_and_extract(zip_urls, download_zip_path=download_csv_path, start_url=start_url)\n",
    "\n",
    "    # read csv to spark dataframe\n",
    "    mentions_df = spark_read_csv(spark_session, csv_filepath=download_csv_path, csv_file_list=extracted_csvs, schema_dictionary=schema_dictionary, csv_type='mentions')\n",
    "\n",
    "    # transform events data\n",
    "    mentions_df = transform_mentions_data(spark_session=spark_session, mentions_df=mentions_df, mongodb_events_collection=mongodb_events_collection, mongodb_database=mongodb_database)\n",
    "    mentions_df.show()\n",
    "    # load events data to mongodb\n",
    "    #load_mongodb(mentions_df, mongodb_database=mongodb_database, mongodb_collection=mongodb_mentions_collection)\n",
    "    mentions_df.write.format('mongodb').option(\"database\",mongodb_database).option(\"collection\", mongodb_collection).mode(\"append\").save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ab4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#etl_mentions(year_list=['2022'], month_list=['01'], day_list=['01','02','03'], schema_dictionary=SCHEMA_DICTIONARY, spark_session=SPARK, mongodb_database='test', mongodb_mentions_collection='mentions', mongodb_events_collection='events', download_csv_path='./gdelt_data/', start_url='http://data.gdeltproject.org/gdeltv2/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad197be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip_urls = get_zip_urls_from_master_filelist(year_list=['2022'], month_list=['01'], day_list=['01','02','03'], zip_type='mentions')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
